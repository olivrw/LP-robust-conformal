{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext lab_black\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, \"backend\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tf_backend.tf_models import BigConvNet, OneLayerNet\n",
    "from tf_backend.tf_losses import pinball_loss_with_scores_keras, dependent_label_quantile_loss_keras, pinball_loss_keras\n",
    "from tf_backend.tf_utils import *\n",
    "from tf_backend.tf_metrics import *\n",
    "from tf_backend.tf_constraints import *\n",
    "\n",
    "from np_backend.numpy_utils import *\n",
    "import np_backend.conformal_utils as cf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.14.0\n",
      "TensorFlow Datasets version:  4.9.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"TensorFlow Datasets version: \", tfds.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5088314075429564584\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6309331024377128616\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9904958725853005179\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"datasets/ImageNet/\"\n",
    "\n",
    "dataset_version = \"testV2/imagenetv2-matched-frequency/\"\n",
    "\n",
    "path_to_scratch = os.getcwd()\n",
    "path_to_data = os.path.join(path_to_scratch, data_folder)\n",
    "\n",
    "n_classes = 1000\n",
    "\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_class_name(file_path):\n",
    "    class_name = tf.strings.split(file_path, '/')[-2]\n",
    "    return tf.strings.to_number(class_name, tf.int64)\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    image = tf.io.read_file(file_path)\n",
    "    label = process_class_name(file_path)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def serialize_example(img, label):\n",
    "    \"\"\"\n",
    "      Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "        'img': _bytes_feature(img),\n",
    "        'label': _int64_feature(label),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    )\n",
    "\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def tf_serialize_example(img, label):\n",
    "    tf_string = tf.py_function(serialize_example, (img, label), tf.string)\n",
    "    return tf.reshape(tf_string, ())\n",
    "\n",
    "\n",
    "def parse_example(img, label):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH]), label\n",
    "\n",
    "\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'img': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    # resize the image to the desired size.\n",
    "    return parse_example(example[\"img\"], example[\"label\"])\n",
    "\n",
    "\n",
    "def one_hot_preprocessing(img, label):\n",
    "    return img, tf.one_hot(tf.reshape(label, [-1]), n_classes, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to tfrecord format (No Need to Run this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testv2_imagenet_files = tf.data.Dataset.list_files(\n",
    "#     path_to_data + dataset_version + \"*/*.*\",\n",
    "# )\n",
    "\n",
    "# dataset_len = 0\n",
    "# for x in testv2_imagenet_files:\n",
    "#     dataset_len += 1\n",
    "# print(\"Number of images: {}\".format(dataset_len))\n",
    "\n",
    "# testv2_imagenet_files = testv2_imagenet_files.map(process_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img, label in testv2_imagenet_files.take(1):\n",
    "#     img, label = parse_example(img, label)\n",
    "#     print(label)\n",
    "#     plt.imshow(img.numpy())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testv2_imagenet_files = testv2_imagenet_files.map(tf_serialize_example)\n",
    "# testv2_imagenet_files = testv2_imagenet_files.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_writer = tf.io.TFRecordWriter(\n",
    "#     path_to_data + dataset_version + \"data.tfrecord\"\n",
    "# )\n",
    "\n",
    "# for i, x in enumerate(testv2_imagenet_files):\n",
    "#     if i % 10 == 0:\n",
    "#         print(i)\n",
    "#     for k in range(x.shape[0]):\n",
    "#         tf_writer.write(x[k].numpy())\n",
    "\n",
    "# tf_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ImageNet Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _processing_imagenet_(example):\n",
    "    img = tf.image.convert_image_dtype(example[\"image\"], tf.float32)\n",
    "    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH]), example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_data = tfds.load(\n",
    "    \"imagenet2012\",\n",
    "    data_dir=path_to_data \"/tensorflow_datasets\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "train_data = imagenet_data[\"train\"]\n",
    "val_data = imagenet_data[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(_processing_imagenet_)\n",
    "train_data = train_data.batch(batch_size=BATCH_SIZE)\n",
    "train_data = train_data.map(one_hot_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.map(_processing_imagenet_)\n",
    "val_data = val_data.batch(batch_size=BATCH_SIZE)\n",
    "val_data = val_data.map(one_hot_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in imagenet_data[\"train\"].map(_processing_imagenet_).take(2):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.imshow(x.numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ImageNet Test V2 Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to download imagenet v2 to a tfrecord dataset before here: https://www.tensorflow.org/datasets/catalog/imagenet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_testv2_tf_record = path_to_data + dataset_version + \"data.tfrecord\"\n",
    "tfrecord_testv2_dataset = tf.data.TFRecordDataset(filename_testv2_tf_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testv2_data = tfrecord_testv2_dataset.map(\n",
    "    _parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in testv2_data.take(2):\n",
    "    print(y)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.imshow(x.numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testv2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testv2_data = testv2_data.batch(batch_size=BATCH_SIZE)\n",
    "testv2_data = testv2_data.map(one_hot_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in testv2_data.take(1):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prediction Model -- Already Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigConvNet(\n",
    "    name=\"InceptionResNetV2\",\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    output_shape=1000,\n",
    "    include_top=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Predictive Network (achieving about 93% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_weights_path = path_to_data + \"/models\" + \"/weights_ResNet50V2CIFAR_pretrained.h5\"\n",
    "# model.load_weights(model_weights_path)\n",
    "\n",
    "optimizer_args = {\"learning_rate\": 0.1, \"momentum\": 0.9, \"nesterov\": True}\n",
    "optimizer = optimizer_dict[\"SGD\"](**optimizer_args)\n",
    "metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")]\n",
    "\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=metrics_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(testv2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Features to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1536\n",
    "model_preprocessing = tf.keras.Model(\n",
    "    inputs=model.input, outputs=model.layers[-2].output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split_list = [\"val\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_datasets = {\"val\": val_data, \"test\": testv2_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "labels = {}\n",
    "features = {}\n",
    "\n",
    "for dataset in imagenet_datasets:\n",
    "    features[dataset] = np.empty((0, n_features))\n",
    "    labels[dataset] = np.empty((0, n_classes))\n",
    "    scores[dataset] = np.empty((0, n_classes))\n",
    "    for x, y in imagenet_datasets[dataset]:\n",
    "        features[dataset] = np.concatenate(\n",
    "            (features[dataset], model_preprocessing(x).numpy()), axis=0\n",
    "        )\n",
    "        scores[dataset] = np.concatenate(\n",
    "            (scores[dataset], np.log(np.maximum(1e-100,\n",
    "                                                model(x).numpy()))),\n",
    "            axis=0\n",
    "        )\n",
    "        labels[dataset] = np.concatenate((labels[dataset], y.numpy()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in imagenet_datasets:\n",
    "    scores[dataset] = np.maximum(-100.0, scores[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_split_list:\n",
    "    np.save(\n",
    "        path_to_data + \"np_data/\" + dataset + \".npy\", {\n",
    "            \"features\": features[dataset],\n",
    "            \"labels\": labels[dataset],\n",
    "            \"scores\": scores[dataset]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load train, val, and testV2 data already preprocessed directly in numpy format, at the following location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maxcauch/Documents/Stanford/phd-research/robust_cv/code/reproducible_code_for_jasa_alnur/datasets/ImageNet/np_data'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{path_to_data}np_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(path_to_data + \"np_data/\" + dataset + \".npy\",\n",
    "        allow_pickle=True).item()[\"features\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
